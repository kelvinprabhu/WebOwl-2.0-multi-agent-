{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15831acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "class KnowledgeRetriever:\n",
    "    def __init__(self, driver, embedder):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            driver: Neo4j driver instance\n",
    "            embedder: SentenceTransformer (or similar) embedding model\n",
    "        \"\"\"\n",
    "        self.driver = driver\n",
    "        self.embedder = embedder\n",
    "        self.faiss_index = None\n",
    "        self.chunk_id_to_index = {}\n",
    "        self.index_to_chunk_id = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb72f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clip-by-openai\n",
      "  Downloading clip_by_openai-1.1-py3-none-any.whl.metadata (369 bytes)\n",
      "Requirement already satisfied: ftfy in /home/kelvin/miniconda3/envs/linux-deep-gpu/lib/python3.12/site-packages (from clip-by-openai) (6.3.1)\n",
      "Requirement already satisfied: regex in /home/kelvin/miniconda3/envs/linux-deep-gpu/lib/python3.12/site-packages (from clip-by-openai) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/kelvin/miniconda3/envs/linux-deep-gpu/lib/python3.12/site-packages (from clip-by-openai) (4.67.1)\n",
      "INFO: pip is looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading clip_by_openai-1.0.1-py3-none-any.whl.metadata (407 bytes)\n",
      "  Downloading clip_by_openai-0.1.1.5-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Downloading clip_by_openai-0.1.1.4-py3-none-any.whl.metadata (8.6 kB)\n",
      "  Downloading clip_by_openai-0.1.1.3-py3-none-any.whl.metadata (8.7 kB)\n",
      "  Downloading clip_by_openai-0.1.1.2-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "  Downloading clip_by_openai-0.1.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "INFO: pip is still looking at multiple versions of clip-by-openai to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[31mERROR: Cannot install clip-by-openai==0.1.0, clip-by-openai==0.1.1, clip-by-openai==0.1.1.2, clip-by-openai==0.1.1.3, clip-by-openai==0.1.1.4, clip-by-openai==0.1.1.5, clip-by-openai==1.0.1 and clip-by-openai==1.1 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "The conflict is caused by:\n",
      "    clip-by-openai 1.1 depends on torch<1.7.2 and >=1.7.1\n",
      "    clip-by-openai 1.0.1 depends on torch<1.7.2 and >=1.7.1\n",
      "    clip-by-openai 0.1.1.5 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.4 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.3 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1.2 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.1 depends on torch==1.7.1\n",
      "    clip-by-openai 0.1.0 depends on torch==1.7.1\n",
      "\n",
      "To fix this you could try to:\n",
      "1. loosen the range of package versions you've specified\n",
      "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
      "\n",
      "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install clip-by-openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de692d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-25 02:24:56.743076: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756088696.959947    4824 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756088697.025210    4824 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756088697.605197    4824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756088697.605259    4824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756088697.605265    4824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756088697.605270    4824 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-25 02:24:57.666307: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'clip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Embedding and ML imports\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mclip\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlipProcessor, BlipForConditionalGeneration\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# LangChain imports\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'clip'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import re\n",
    "import io\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import hashlib\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Embedding and ML imports\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import clip\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Graph database (Neo4j example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from neo4j import GraphDatabase\n",
    "except ImportError:\n",
    "    print(\"Warning: neo4j not installed. Graph DB functionality will be limited.\")\n",
    "    GraphDatabase = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ContentNode:\n",
    "    \"\"\"Structured content node for graph database\"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    content_type: str  # 'page', 'pdf', 'image', 'table'\n",
    "    text_content: Optional[str] = None\n",
    "    image_urls: Optional[List[str]] = None\n",
    "    metadata: Optional[Dict[str, Any]] = None\n",
    "    embeddings: Optional[Dict[str, np.ndarray]] = None\n",
    "    relationships: Optional[List[str]] = None\n",
    "    timestamp: str = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.timestamp is None:\n",
    "            self.timestamp = datetime.now().isoformat()\n",
    "\n",
    "\n",
    "class MultimodalEmbeddings:\n",
    "    \"\"\"Handle different types of embeddings for multimodal content\"\"\"\n",
    "    \n",
    "    def __init__(self, device=\"cpu\"):\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize models\n",
    "        self.text_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "        \n",
    "        # BLIP for image captioning\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "        \n",
    "    def encode_text(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Encode text using sentence transformers\"\"\"\n",
    "        if not texts:\n",
    "            return np.array([])\n",
    "        return self.text_model.encode(texts, convert_to_numpy=True)\n",
    "    \n",
    "    def encode_images(self, image_urls: List[str]) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Encode images using CLIP and generate captions with BLIP\"\"\"\n",
    "        if not image_urls:\n",
    "            return np.array([]), []\n",
    "            \n",
    "        embeddings = []\n",
    "        captions = []\n",
    "        \n",
    "        for img_url in image_urls:\n",
    "            try:\n",
    "                # Download image\n",
    "                response = requests.get(img_url, timeout=10, \n",
    "                                      headers={\"User-Agent\": \"MultimodalCrawler/1.0\"})\n",
    "                if response.status_code != 200:\n",
    "                    continue\n",
    "                    \n",
    "                image = Image.open(io.BytesIO(response.content)).convert('RGB')\n",
    "                \n",
    "                # CLIP embedding\n",
    "                image_tensor = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    clip_embedding = self.clip_model.encode_image(image_tensor)\n",
    "                    embeddings.append(clip_embedding.cpu().numpy().flatten())\n",
    "                \n",
    "                # BLIP caption\n",
    "                inputs = self.blip_processor(image, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    caption_ids = self.blip_model.generate(**inputs, max_length=50)\n",
    "                    caption = self.blip_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "                    captions.append(caption)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process image {img_url}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        return np.array(embeddings), captions\n",
    "    \n",
    "    def encode_multimodal(self, text: str, image_urls: List[str]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Combine text and image embeddings\"\"\"\n",
    "        embeddings = {}\n",
    "        \n",
    "        if text and len(text.strip()) > 0:\n",
    "            embeddings['text'] = self.encode_text([text])[0]\n",
    "            \n",
    "        if image_urls:\n",
    "            img_embeddings, captions = self.encode_images(image_urls)\n",
    "            if len(img_embeddings) > 0:\n",
    "                embeddings['images'] = img_embeddings\n",
    "                embeddings['captions'] = self.encode_text(captions) if captions else np.array([])\n",
    "                \n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class GraphDatabaseManager:\n",
    "    \"\"\"Manage graph database operations\"\"\"\n",
    "    \n",
    "    def __init__(self, uri=\"bolt://localhost:7687\", user=\"neo4j\", password=\"password\"):\n",
    "        if GraphDatabase is None:\n",
    "            print(\"Neo4j driver not available. Graph operations will be skipped.\")\n",
    "            self.driver = None\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to Neo4j: {e}\")\n",
    "            self.driver = None\n",
    "    \n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "    \n",
    "    def create_content_node(self, node: ContentNode):\n",
    "        \"\"\"Create a content node in the graph database\"\"\"\n",
    "        if not self.driver:\n",
    "            return\n",
    "            \n",
    "        with self.driver.session() as session:\n",
    "            session.write_transaction(self._create_node, node)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_node(tx, node: ContentNode):\n",
    "        query = \"\"\"\n",
    "        MERGE (n:Content {url: $url})\n",
    "        SET n.title = $title,\n",
    "            n.content_type = $content_type,\n",
    "            n.text_content = $text_content,\n",
    "            n.timestamp = $timestamp\n",
    "        \"\"\"\n",
    "        tx.run(query, **asdict(node))\n",
    "    \n",
    "    def create_relationship(self, from_url: str, to_url: str, relationship_type: str):\n",
    "        \"\"\"Create relationship between nodes\"\"\"\n",
    "        if not self.driver:\n",
    "            return\n",
    "            \n",
    "        with self.driver.session() as session:\n",
    "            session.write_transaction(self._create_relationship, from_url, to_url, relationship_type)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_relationship(tx, from_url: str, to_url: str, rel_type: str):\n",
    "        query = \"\"\"\n",
    "        MATCH (a:Content {url: $from_url})\n",
    "        MATCH (b:Content {url: $to_url})\n",
    "        MERGE (a)-[r:LINKS_TO {type: $rel_type}]->(b)\n",
    "        \"\"\"\n",
    "        tx.run(query, from_url=from_url, to_url=to_url, rel_type=rel_type)\n",
    "\n",
    "\n",
    "class LangChainRetriever:\n",
    "    \"\"\"LangChain integration for retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model: MultimodalEmbeddings):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        self.vectorstore = None\n",
    "        \n",
    "    def create_vectorstore(self, documents: List[ContentNode], persist_directory=\"./chroma_db\"):\n",
    "        \"\"\"Create vector store from content nodes\"\"\"\n",
    "        langchain_docs = []\n",
    "        \n",
    "        for node in documents:\n",
    "            if node.text_content:\n",
    "                # Split text into chunks\n",
    "                chunks = self.text_splitter.split_text(node.text_content)\n",
    "                \n",
    "                for i, chunk in enumerate(chunks):\n",
    "                    doc = Document(\n",
    "                        page_content=chunk,\n",
    "                        metadata={\n",
    "                            \"url\": node.url,\n",
    "                            \"title\": node.title,\n",
    "                            \"content_type\": node.content_type,\n",
    "                            \"chunk_id\": i,\n",
    "                            \"timestamp\": node.timestamp\n",
    "                        }\n",
    "                    )\n",
    "                    langchain_docs.append(doc)\n",
    "        \n",
    "        # Custom embedding wrapper\n",
    "        class CustomEmbeddings(Embeddings):\n",
    "            def __init__(self, model):\n",
    "                self.model = model\n",
    "                \n",
    "            def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "                embeddings = self.model.encode_text(texts)\n",
    "                return [emb.tolist() for emb in embeddings]\n",
    "                \n",
    "            def embed_query(self, text: str) -> List[float]:\n",
    "                embedding = self.model.encode_text([text])[0]\n",
    "                return embedding.tolist()\n",
    "        \n",
    "        custom_embeddings = CustomEmbeddings(self.embedding_model)\n",
    "        \n",
    "        self.vectorstore = Chroma.from_documents(\n",
    "            documents=langchain_docs,\n",
    "            embedding=custom_embeddings,\n",
    "            persist_directory=persist_directory\n",
    "        )\n",
    "        \n",
    "        return self.vectorstore\n",
    "    \n",
    "    def similarity_search(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Perform similarity search\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        return self.vectorstore.similarity_search(query, k=k)\n",
    "\n",
    "\n",
    "class EnhancedWebCrawler:\n",
    "    \"\"\"Enhanced web crawler with multimodal embeddings and graph DB integration\"\"\"\n",
    "    \n",
    "    def __init__(self, use_gpu=False, neo4j_config=None):\n",
    "        device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        self.embeddings = MultimodalEmbeddings(device=device)\n",
    "        \n",
    "        # Initialize graph database\n",
    "        if neo4j_config:\n",
    "            self.graph_db = GraphDatabaseManager(**neo4j_config)\n",
    "        else:\n",
    "            self.graph_db = GraphDatabaseManager()\n",
    "        \n",
    "        # Initialize retriever\n",
    "        self.retriever = LangChainRetriever(self.embeddings)\n",
    "        \n",
    "        # Store processed nodes\n",
    "        self.content_nodes: List[ContentNode] = []\n",
    "    \n",
    "    def extract_pdf_data(self, pdf_url: str) -> Optional[Dict]:\n",
    "        \"\"\"Enhanced PDF extraction with embeddings\"\"\"\n",
    "        try:\n",
    "            resp = requests.get(pdf_url, timeout=10, \n",
    "                              headers={\"User-Agent\": \"EnhancedCrawler/1.0\"})\n",
    "            if resp.status_code != 200:\n",
    "                return None\n",
    "\n",
    "            pdf_file = io.BytesIO(resp.content)\n",
    "            reader = PdfReader(pdf_file)\n",
    "\n",
    "            # Extract metadata\n",
    "            title = reader.metadata.title if reader.metadata and reader.metadata.title else None\n",
    "            if not title:\n",
    "                parsed_url = urlparse(pdf_url)\n",
    "                filename = os.path.basename(parsed_url.path)\n",
    "                title = filename if filename else \"Untitled PDF\"\n",
    "\n",
    "            # Extract text\n",
    "            text_content = []\n",
    "            for page in reader.pages:\n",
    "                try:\n",
    "                    text_content.append(page.extract_text() or \"\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "            text = \"\\n\".join(text_content).strip()\n",
    "\n",
    "            # Generate embeddings\n",
    "            embeddings = {}\n",
    "            if text:\n",
    "                embeddings = self.embeddings.encode_multimodal(text, [])\n",
    "\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"download_link\": pdf_url,\n",
    "                \"content\": text if text else None,\n",
    "                \"embeddings\": embeddings\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to parse PDF {pdf_url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def extract_enhanced_content(self, url: str, soup: BeautifulSoup) -> ContentNode:\n",
    "        \"\"\"Extract content with multimodal embeddings\"\"\"\n",
    "        \n",
    "        # Get title\n",
    "        title_tag = soup.find(\"title\")\n",
    "        title = title_tag.get_text().strip() if title_tag else urlparse(url).path\n",
    "        \n",
    "        # Clean text content\n",
    "        for script in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text(\" \", strip=True)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        text_content = text if len(text) > 50 else None\n",
    "\n",
    "        # Collect images\n",
    "        image_urls = []\n",
    "        for img in soup.find_all(\"img\", src=True):\n",
    "            img_url = urljoin(url, img[\"src\"])\n",
    "            image_urls.append(img_url)\n",
    "\n",
    "        # Generate multimodal embeddings\n",
    "        embeddings = self.embeddings.encode_multimodal(\n",
    "            text_content or \"\", \n",
    "            image_urls[:5]  # Limit to first 5 images for performance\n",
    "        )\n",
    "\n",
    "        # Collect additional metadata\n",
    "        metadata = {\n",
    "            \"images\": image_urls,\n",
    "            \"tables\": [str(table) for table in soup.find_all(\"table\")],\n",
    "            \"pdfs\": [],\n",
    "            \"other_files\": []\n",
    "        }\n",
    "\n",
    "        # Process file links\n",
    "        for a in soup.find_all(\"a\", href=True):\n",
    "            href = urljoin(url, a[\"href\"])\n",
    "            if href.lower().endswith(\".pdf\"):\n",
    "                pdf_data = self.extract_pdf_data(href)\n",
    "                if pdf_data:\n",
    "                    metadata[\"pdfs\"].append(pdf_data)\n",
    "            elif re.search(r\"\\.(zip|docx?|xlsx?|pptx?)$\", href, re.I):\n",
    "                metadata[\"other_files\"].append(href)\n",
    "\n",
    "        # Create content node\n",
    "        node = ContentNode(\n",
    "            url=url,\n",
    "            title=title,\n",
    "            content_type=\"page\",\n",
    "            text_content=text_content,\n",
    "            image_urls=image_urls if image_urls else None,\n",
    "            metadata=metadata,\n",
    "            embeddings=embeddings\n",
    "        )\n",
    "\n",
    "        return node\n",
    "\n",
    "    def crawl_site_tree(self, start_url: str, max_pages: int = 30) -> Dict[str, ContentNode]:\n",
    "        \"\"\"Enhanced crawling with graph relationships\"\"\"\n",
    "        domain = urlparse(start_url).netloc\n",
    "        visited = set()\n",
    "        crawl_tree = {}\n",
    "\n",
    "        queue = deque([start_url])\n",
    "\n",
    "        while queue and len(visited) < max_pages:\n",
    "            url = queue.popleft()\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url, \n",
    "                                  headers={\"User-Agent\": \"EnhancedCrawler/1.0\"}, \n",
    "                                  timeout=10)\n",
    "                if resp.status_code != 200 or \"text/html\" not in resp.headers.get(\"Content-Type\", \"\"):\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to crawl {url}: {e}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # Extract enhanced content\n",
    "            content_node = self.extract_enhanced_content(url, soup)\n",
    "            self.content_nodes.append(content_node)\n",
    "\n",
    "            # Create graph node\n",
    "            self.graph_db.create_content_node(content_node)\n",
    "\n",
    "            # Extract and process child links\n",
    "            children = []\n",
    "            for a in soup.find_all(\"a\", href=True):\n",
    "                absolute = urljoin(url, a[\"href\"])\n",
    "                anchor_text = a.get_text(\" \", strip=True)\n",
    "\n",
    "                if (urlparse(absolute).netlnet != domain or \n",
    "                    absolute in visited or \n",
    "                    not anchor_text or anchor_text.isspace()):\n",
    "                    continue\n",
    "\n",
    "                children.append({\n",
    "                    \"url\": absolute,\n",
    "                    \"anchor_text\": anchor_text\n",
    "                })\n",
    "                \n",
    "                # Create relationship in graph\n",
    "                self.graph_db.create_relationship(url, absolute, \"LINKS_TO\")\n",
    "                \n",
    "                queue.append(absolute)\n",
    "\n",
    "            content_node.relationships = [child[\"url\"] for child in children]\n",
    "            crawl_tree[url] = content_node\n",
    "\n",
    "        return crawl_tree\n",
    "\n",
    "    def create_retrieval_system(self, persist_directory=\"./chroma_db\"):\n",
    "        \"\"\"Create the retrieval system using LangChain\"\"\"\n",
    "        vectorstore = self.retriever.create_vectorstore(\n",
    "            self.content_nodes, \n",
    "            persist_directory\n",
    "        )\n",
    "        return vectorstore\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Document]:\n",
    "        \"\"\"Search the crawled content\"\"\"\n",
    "        return self.retriever.similarity_search(query, k)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.graph_db.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2eb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    neo4j_config = {\n",
    "        \"uri\": \"bolt://localhost:7687\",\n",
    "        \"user\": \"neo4j\", \n",
    "        \"password\": \"password\"\n",
    "    }\n",
    "    \n",
    "    # Initialize crawler\n",
    "    crawler = EnhancedWebCrawler(\n",
    "        use_gpu=True,  # Set to False if no GPU\n",
    "        neo4j_config=neo4j_config\n",
    "    )\n",
    "    \n",
    "    # Crawl website\n",
    "    site_map = crawler.crawl_site_tree(\"https://example.com\", max_pages=10)\n",
    "    \n",
    "    # Create retrieval system\n",
    "    vectorstore = crawler.create_retrieval_system()\n",
    "    \n",
    "    # Example search\n",
    "    results = crawler.search(\"machine learning algorithms\", k=3)\n",
    "    for result in results:\n",
    "        print(f\"URL: {result.metadata['url']}\")\n",
    "        print(f\"Content: {result.page_content[:200]}...\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    # Clean up\n",
    "    crawler.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linux-deep-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
